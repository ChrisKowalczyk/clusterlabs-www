---
layout: pacemaker
title: ClusterLabs - Pacemaker
---

 <section id="boxes">
   <aside>
     <h2>Deploy</h2>
     <a href="{% asset_path Deploy.png %}">{% img Deploy-small.png %}</a>
     <p>
       We support many deployment scenarios, from the simplest
       2-node standby cluster to a 32-node active/active
       configuration.

       We can also dramatically reduce hardware costs by allowing
       several active/passive clusters to be combined and share a common
       backup node.
     </p>
   </aside>

   <aside>
     <h2>Monitor</h2>
     <a href="{% asset_path Monitor.png %}">{% img Monitor-small.png %}</a>
     <p>
       We monitor the system for both hardware and software failures.

       In the event of a failure, we will automatically recover
       your application and make sure it is available from one
       of the remaning machines in the cluster.
     </p>
   </aside>
   
   <aside>
     <h2>Recover</h2>
     <a href="{% asset_path Recover.png %}">{% img Recover-small.png %}</a>
     <p>
       After a failure, we use advanced algorithms to quickly
       determine the optimum locations for services based on
       relative node preferences and/or requirements to run with
       other cluster services (we call these "constraints").
     </p>
   </aside>
 </section>

  <section id="main">
    <a id="why"> </a>
    
    <p>
      At its core, a cluster is a distributed finite state
      machine capable of co-ordinating the startup and recovery
      of inter-related services across a set of machines.
    </p>
    <blockquote class="pullquote">
      System HA is possible without a cluster manager, but you save many headaches using one anyway
    </blockquote>
    <p>
      Even a distributed and/or replicated application that is
      able to survive the failure of one or more components can
      benefit from a higher level cluster:
      <ul>
        <li>awareness of other applications in the stack</li>
        <li>a shared <a href="https://en.wikipedia.org/wiki/Quorum_%28Distributed_Systems%29">quorum</a> implementation and calculation</li>
        <li>data integrity through fencing (a non-responsive process does not imply it is not doing anything)</li>
        <li>automated recovery of instances to ensure capacity</li>
      </ul>
    </p>
    <p>
      While SYS-V init replacements like systemd can provide
      deterministic recovery of a complex stack of services, the
      recovery is limited to one machine and lacks the context
      of what is happening on other machines - context that is
      crucial to determine the difference between a local
      failure, clean startup or recovery after a total site
      failure.
    </p>

    <a id="features"> </a>
    <h2>Features</h2>
    <p>
      The ClusterLabs stack, incorporating <a href="">Corosync</a>
      and <a href="/pacemaker.html">Pacemaker</a> defines
      an <a href="https://en.wikipedia.org/wiki/Open_source">Open Source</a>,
      <a href="https://en.wikipedia.org/wiki/High_availability">High Availability</a>
       <a href="https://en.wikipedia.org/wiki/Cluster_(computing)">cluster</a>
       offering suitable for both small and large deployments.
    </p>

    <ul>
      <li>Detection and recovery of machine and application-level failures</li>
      <li>Supports practically any <a href="http://wiki.clusterlabs.org/wiki/ClusterTypes">redundancy configuration</a></li>
      <li>Supports both <a href="https://en.wikipedia.org/wiki/Quorum_%28Distributed_Systems%29">quorate</a> and <a href="https://web.archive.org/web/20110727185030/http://devresources.linux-foundation.org/dev/clusters/docs/ResourceDrivenClusters.pdf">resource-driven</a> clusters</li>
      <li>Configurable <a href="/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-cluster-options.html#id36137161">strategies</a> for dealing with <a href="https://en.wikipedia.org/wiki/Quorum_%28Distributed_Systems%29">quorum</a> loss (when multiple machines fail)</li>
      <li>Supports application <a href="/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-resource-ordering.html">startup/shutdown ordering</a>, regardless of which machine(s) the applications are on</li>
      <li>Supports applications that must/must-not run on the <a href="/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-resource-colocation.html">same machine</a></li>
      <li>Supports applications which need to be active on <a href="/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-resource-clone.html">multiple machines</a></li>
      <li>Supports applications with multiple modes (eg. <a href="/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-resource-multistate.html">master/slave</a>)<br/><br/></li>
      <li>Provably correct response to any failure or cluster state.<br/>
  The cluster's response to any stimuli can be tested offline <strong><i>before</i></strong> the condition exists
      </li>
    </ul>

    <h2>Components</h2>
    <blockquote class="pullquote">
    "The definitive open-source high-availability stack for the Linux
    platform builds upon the Pacemaker cluster resource manager."<br />
    -- <cite>LINUX Journal</cite>,
    <a href="http://www.linuxjournal.com/content/ahead-pack-pacemaker-high-availability-stack">"Ahead
    of the Pack: the Pacemaker High-Availability Stack"</a>
    </blockquote>
    <p>A Pacemaker stack is built on five core components:  
    <ul>
      <li>libQB - core services (logging, IPC, etc)</li>
      <li>Corosync - Membership, messaging and quorum</li>
      <li>Resource agents - A collection of scripts that interact with the underlying services managed by the cluster</li>
      <li>Fencing agents - A collection of scripts that interact with network power switches and SAN devices to isolate cluster members</li>
      <li>Pacemaker itself</li>
    </ul>
    </p>

    <p>
      We describe each of these in <a href="/components.html">more detail</a> as well as other optional components such as CLIs and GUIs.
    </p>

    <h2>Background</h2>
    <p>
      Pacemaker has been around
      since <a href="https://www.ohloh.net/p/pacemaker/analyses/latest/languages_summary">2004</a>
      and is primarily a collaborative effort
      between <a href="http://www.redhat.com">Red Hat</a>
      and <a href="http://www.suse.com">SUSE</a>, however we also
      receive considerable help and support from the folks
      at <a href="http://www.linbit.com">LinBit</a> and the community in
      general.
    </p>
    <blockquote class="pullquote">
     "Pacemaker cluster stack is the state-of-the-art high availability
     and load balancing stack for the Linux platform."<br />
     -- <a href="http://docs.openstack.org/ha-guide/controller-ha-pacemaker.html"><cite>OpenStack
     documentation</cite></a>
    </blockquote>
    <p>
      Corosync also began life in <a href="https://www.ohloh.net/p/corosync/analyses/latest/languages_summary">2004</a>
      but was then part of the <a href="https://www.ohloh.net/p/openAIS">OpenAIS project</a>.
      It is primarily a <a href="http://www.redhat.com">Red
      Hat</a> initiative, however we also receive considerable
      help and support from the folks in the community.
    </p>
    <p>
      The core ClusterLabs team is made up of full-time
      developers from Australia, Austria, Canada, China, Czech
      Repulic, England, Germany, Sweden and the USA.  Contributions to
      the code or documentation are always welcome.
    </p>
    <p>
      The ClusterLabs stack ships with most modern enterprise
      distributions and has been deployed in many critical
      environments including Deutsche Flugsicherung GmbH
      (<a href="http://www.dfs.de/dfs_homepage/en/">DFS</a>)
      which uses Pacemaker to ensure
      its <a href="http://www.novell.com/success/dfs.html">air
      traffic control systems</a> are always available.
    </p>
  </section>

